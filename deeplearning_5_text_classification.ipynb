{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b50dd8bb",
   "metadata": {},
   "source": [
    "# Title: deep learning part 5, text classification\n",
    "\n",
    "## Aim:\n",
    "\n",
    "+ learning text classification\n",
    "+ practising jupyter lab in vs code\n",
    "\n",
    "Ref: https://www.tensorflow.org/text/guide/word_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb266b5",
   "metadata": {},
   "source": [
    "## Text embedding\n",
    "\n",
    " + one-hot encoding. inefficient. sparse matrix\n",
    "\n",
    " + represent with unique integer for each word. efficient, dense populated matrix, but captures no relationship between words.\n",
    "\n",
    " + word embedding. capture relationship and no need to do it manually. The embedding is present as an extra layer and embedding is learning as a part of training. \n",
    "\n",
    "We only need to input the dimension (the length of each word encoding.8, for a small dataset and 1024 for a large dataset)\n",
    "\n",
    "e.g., 4-dimensional word embedding\n",
    "\n",
    "    cat: [2.1, 1.5 -0.5,4]\n",
    "    mat: [1.8,-0.5, 0,-1]\n",
    "    .....\n",
    "\n",
    "they float numbers.\n",
    "\n",
    "next, let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf11e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "print(f\"tensorflow version:{tf.__version__}\")\n",
    "print(f\"gpu:{tf.config.list_logical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc017c63",
   "metadata": {},
   "source": [
    "Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d028e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb_v1_extracted/aclImdb')\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb62bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "seed = 123\n",
    "print(dataset_dir)\n",
    "train_dir=os.path.join(dataset_dir,'train')\n",
    "\n",
    "print(train_dir)\n",
    "\n",
    "print(os.listdir(train_dir))\n",
    "assert(os.path.isdir(train_dir))\n",
    "\n",
    "#remove unsup from the trainning folder\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "\n",
    "\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir,'train'), batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir,'train'), batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n",
    "\n",
    "print(\"type of train_ds\", type(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show some files\n",
    "count=0\n",
    "for text_batch, label_batch in train_ds: #.take(1):\n",
    "  print(type(text_batch))\n",
    "  print(text_batch.shape)\n",
    "  count+=1\n",
    "  if count >1:\n",
    "    break;\n",
    "  \n",
    "  print(label_batch.numpy(), text_batch.numpy())\n",
    "\n",
    "\n",
    "\n",
    "#configure\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833cf25",
   "metadata": {},
   "source": [
    "## Embedding layer\n",
    "\n",
    "\n",
    "    The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n",
    "\n",
    "    Let's see an example with a model of one single embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(1000, 64))\n",
    "# The model will take as input an integer matrix of size (batch,\n",
    "# input_length), and the largest integer (i.e. word index) in the input\n",
    "# should be no larger than 999 (vocabulary size).\n",
    "# Now model.output_shape is (None, 10, 64), where `None` is the batch\n",
    "# dimension.\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array.shape)\n",
    "\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play to show some input output\n",
    "\n",
    "print(input_array[0:2])\n",
    "\n",
    "print(output_array[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49df8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play to have word input??\n",
    "# NO NO, this can not work!! Embedding only takes\n",
    "# in integers as input. that is what Textvectorize function\n",
    "# does!!! \n",
    "input_words=[['I','like','you'],\n",
    "    ['he','does','hate']\n",
    "]\n",
    "\n",
    "#commented, since it can not work!!\n",
    "tf_array=tf.constant(input_words)\n",
    "\n",
    "print(tf_array)\n",
    "\n",
    "#output_words=model.predict(tf_array)\n",
    "\n",
    "print(\"WE NEED TO VECTERIZE TEXT INTO INTEGERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814b8e0",
   "metadata": {},
   "source": [
    "Next, we want to play more with the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a75405",
   "metadata": {},
   "source": [
    "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation.\n",
    "\n",
    "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table. This is how to retrieve the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9858e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "\n",
    "print(result.numpy())\n",
    "\n",
    "result2 = embedding_layer(tf.constant([1,2,2]))\n",
    "\n",
    "print(result2) #check for duplicated entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3050c7e",
   "metadata": {},
   "source": [
    "Notes: **Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument:**\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a (2, 3) input batch and the output is (2, 3, N)\n",
    "\n",
    "Note: in this case, the first axis is the batch dimension!!!\n",
    "\n",
    "When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2], [2, 4, 5]]))\n",
    "result.shape\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0463cd9",
   "metadata": {},
   "source": [
    "Text preprocessing\n",
    "\n",
    "we do two things here, standardize/normalize texts, and then vectorize them.\n",
    "\n",
    "standardize text: means to lower case all the texts, strip them of HTML tag, and remove punctuations. so far that is all.\n",
    "\n",
    "vectorize : means to turn texts into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "print(type(text_ds), \" and \", type(train_ds.take(-1)))\n",
    "xx=train_ds.take(-1)\n",
    "\n",
    "count=1\n",
    "for element in train_ds:\n",
    "  print(\"--\",element)\n",
    "  count+=1\n",
    "  if count >1:\n",
    "    break\n",
    "print(\"=---------===\")\n",
    "print(text_ds.element_spec, \" and \", train_ds.element_spec)\n",
    "#print(f\"train_ds shape:{train_ds.shape} and \\n text_ds shape:{text_ds.shape}\")\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d5fd5",
   "metadata": {},
   "source": [
    "start building model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500093e1",
   "metadata": {},
   "source": [
    "The TextVectorization layer transforms strings into vocabulary indices. You have already initialized vectorize_layer as a TextVectorization layer and built its vocabulary by calling adapt on text_ds. Now vectorize_layer can be used as the first layer of your end-to-end classification model, feeding transformed strings into the Embedding layer.\n",
    "The Embedding layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n",
    "\n",
    "The GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
    "\n",
    "The fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n",
    "\n",
    "The last layer is densely connected with a single output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15\n",
    "    #callbacks=[tensorboard_callback]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
