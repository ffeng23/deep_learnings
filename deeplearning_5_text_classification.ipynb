{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b50dd8bb",
   "metadata": {},
   "source": [
    "# Title: deep learning part 5, text classification\n",
    "\n",
    "## Aim:\n",
    "\n",
    "+ learning text classification\n",
    "+ practising jupyter lab in vs code\n",
    "\n",
    "Ref: https://www.tensorflow.org/text/guide/word_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb266b5",
   "metadata": {},
   "source": [
    "## Text embedding\n",
    "\n",
    " + one-hot encoding. inefficient. sparse matrix\n",
    "\n",
    " + represent with unique integer for each word. efficient, dense populated matrix, but captures no relationship between words.\n",
    "\n",
    " + word embedding. capture relationship and no need to do it manually. The embedding is present as an extra layer and embedding is learning as a part of training. \n",
    "\n",
    "We only need to input the dimension (the length of each word encoding.8, for a small dataset and 1024 for a large dataset)\n",
    "\n",
    "e.g., 4-dimensional word embedding\n",
    "\n",
    "    cat: [2.1, 1.5 -0.5,4]\n",
    "    mat: [1.8,-0.5, 0,-1]\n",
    "    .....\n",
    "\n",
    "they float numbers.\n",
    "\n",
    "next, let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf11e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "print(f\"tensorflow version:{tf.__version__}\")\n",
    "print(f\"gpu:{tf.config.list_logical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc017c63",
   "metadata": {},
   "source": [
    "Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d028e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb_v1_extracted/aclImdb')\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb62bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "seed = 123\n",
    "print(dataset_dir)\n",
    "train_dir=os.path.join(dataset_dir,'train')\n",
    "\n",
    "print(train_dir)\n",
    "\n",
    "print(os.listdir(train_dir))\n",
    "assert(os.path.isdir(train_dir))\n",
    "\n",
    "#remove unsup from the trainning folder\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "\n",
    "\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir,'train'), batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir,'train'), batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n",
    "\n",
    "print(\"type of train_ds\", type(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show some files\n",
    "count=0\n",
    "for text_batch, label_batch in train_ds: #.take(1):\n",
    "  print(type(text_batch))\n",
    "  print(text_batch.shape)\n",
    "  count+=1\n",
    "  if count >1:\n",
    "    break;\n",
    "  \n",
    "  print(label_batch.numpy(), text_batch.numpy())\n",
    "\n",
    "\n",
    "\n",
    "#configure\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833cf25",
   "metadata": {},
   "source": [
    "## Embedding layer\n",
    "\n",
    "\n",
    "    The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n",
    "\n",
    "    Let's see an example with a model of one single embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(1000, 64))\n",
    "# The model will take as input an integer matrix of size (batch,\n",
    "# input_length), and the largest integer (i.e. word index) in the input\n",
    "# should be no larger than 999 (vocabulary size).\n",
    "# Now model.output_shape is (None, 10, 64), where `None` is the batch\n",
    "# dimension.\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array.shape)\n",
    "\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play to show some input output\n",
    "\n",
    "print(input_array[0:2])\n",
    "\n",
    "print(output_array[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49df8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play to have word input??\n",
    "# NO NO, this can not work!! Embedding only takes\n",
    "# in integers as input. that is what Textvectorize function\n",
    "# does!!! \n",
    "input_words=[['I','like','you'],\n",
    "    ['he','does','hate']\n",
    "]\n",
    "\n",
    "#commented, since it can not work!!\n",
    "tf_array=tf.constant(input_words)\n",
    "\n",
    "print(tf_array)\n",
    "\n",
    "#output_words=model.predict(tf_array)\n",
    "\n",
    "print(\"WE NEED TO VECTERIZE TEXT INTO INTEGERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814b8e0",
   "metadata": {},
   "source": [
    "Next, we want to play more with the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a75405",
   "metadata": {},
   "source": [
    "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation.\n",
    "\n",
    "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table. This is how to retrieve the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9858e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "\n",
    "print(result.numpy())\n",
    "\n",
    "result2 = embedding_layer(tf.constant([1,2,2]))\n",
    "\n",
    "print(result2) #check for duplicated entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3050c7e",
   "metadata": {},
   "source": [
    "Notes: **Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument:**\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a (2, 3) input batch and the output is (2, 3, N)\n",
    "\n",
    "Note: in this case, the first axis is the batch dimension!!!\n",
    "\n",
    "When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2], [2, 4, 5]]))\n",
    "result.shape\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0463cd9",
   "metadata": {},
   "source": [
    "Text preprocessing\n",
    "\n",
    "we do two things here, standardize/normalize texts, and then vectorize them.\n",
    "\n",
    "standardize text: means to lower case all the texts, strip them of HTML tag, and remove punctuations. so far that is all.\n",
    "\n",
    "vectorize : means to turn texts into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "print(type(text_ds), \" and \", type(train_ds.take(-1)))\n",
    "xx=train_ds.take(-1)\n",
    "\n",
    "count=1\n",
    "for element in train_ds:\n",
    "  print(\"--\",element)\n",
    "  count+=1\n",
    "  if count >1:\n",
    "    break\n",
    "print(\"=---------===\")\n",
    "print(text_ds.element_spec, \" and \", train_ds.element_spec)\n",
    "#print(f\"train_ds shape:{train_ds.shape} and \\n text_ds shape:{text_ds.shape}\")\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d5fd5",
   "metadata": {},
   "source": [
    "start building model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500093e1",
   "metadata": {},
   "source": [
    "The TextVectorization layer transforms strings into vocabulary indices. You have already initialized vectorize_layer as a TextVectorization layer and built its vocabulary by calling adapt on text_ds. Now vectorize_layer can be used as the first layer of your end-to-end classification model, feeding transformed strings into the Embedding layer.\n",
    "The Embedding layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n",
    "\n",
    "The GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
    "\n",
    "The fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n",
    "\n",
    "The last layer is densely connected with a single output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15\n",
    "    #callbacks=[tensorboard_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affe062",
   "metadata": {},
   "source": [
    "## Things to learn\n",
    "\n",
    "### tf.data.dataset\n",
    "\n",
    "        Each element in the\n",
    "        tf.data.Dataset object returned by tf.keras.utils.text_dataset_from_directory is a batch of data, the structure of which depends on the label_mode parameter. \n",
    "        Dataset Element Structure\n",
    "\n",
    "            If label_mode is None: Each element in the dataset is a single string tensor of shape (batch_size,), containing the raw contents of a batch of text files.\n",
    "            If label_mode is set (e.g., 'int', 'categorical', or 'binary'): Each element is a tuple (texts, labels), where:\n",
    "                texts is a string tensor of shape (batch_size,) containing the raw text content from the files.\n",
    "                labels is a tensor of shape (batch_size,) (or (batch_size, num_classes) for 'categorical' mode) containing the corresponding labels for each text in the batch. The labels are typically inferred from the subdirectory names. \n",
    "\n",
    "        Key Parameters Affecting Output\n",
    "\n",
    "            batch_size: The dataset yields batches of data, not individual samples. The batch_size argument (defaulting to 32) determines the number of samples in each batch.\n",
    "            label_mode: This parameter determines how labels are encoded (e.g., as integers, binary, or categorical vectors) or if they are included at all.\n",
    "            labels: If set to 'inferred' (default), the directory structure is used to automatically generate labels. For example, files in main_directory/class_a/ get label 0, and files in main_directory/class_b/ get label 1. \n",
    "\n",
    "        The tf.data.Dataset is designed for building efficient input pipelines in TensorFlow, handling large amounts of data by processing it in batches and allowing for various transformations like shuffling, batching, and caching\n",
    "\n",
    "#### tf.data.dataset.take\n",
    "\n",
    "\n",
    "        The tf.data.Dataset.take(count) function in TensorFlow is used to create a new dataset that contains a specified number of elements from the beginning of the original dataset.\n",
    "        Functionality:\n",
    "\n",
    "            Subset Creation:\n",
    "            It extracts the first count elements from the tf.data.Dataset it is called upon.\n",
    "            New Dataset:\n",
    "            It returns a new tf.data.Dataset object containing only these count elements. The original dataset remains unchanged.\n",
    "            Handling count = -1 or count > dataset_size:\n",
    "            If count is set to -1, or if the specified count is greater than the total number of elements in the dataset, the take() function will effectively return a new dataset containing all elements of the original dataset. \n",
    "\n",
    "\n",
    "### regex\n",
    "\n",
    "regex, tf.string.regex\n",
    "\n",
    "other python regular expression\n",
    "import re\n",
    "\n",
    "pyhton regular expression is similar to R regular expression\n",
    "\n",
    "there are other re too using % style.\n",
    "\n",
    "\n",
    "        TensorFlow's tf.strings.regex_replace and tf.strings.regex_full_match functions expect a regular expression pattern as a string. While Python's re module allows for various ways to construct and format regular expressions, when passing them to TensorFlow functions, the pattern must be a plain string.\n",
    "        The % style format, also known as old-style string formatting, is a Python feature for creating strings with embedded values. While you can use it to construct a regular expression string, it's generally recommended to use f-strings (formatted string literals) for better readability and modern Python practices when creating dynamic regex patterns.\n",
    "        Here's how you can use % style formatting to create a regex pattern string for use with TensorFlow:\n",
    "        Python\n",
    "\n",
    "        import tensorflow as tf\n",
    "\n",
    "        # Define variables for the regex pattern\n",
    "        num_digits = 3\n",
    "        start_char = \"A\"\n",
    "\n",
    "        # Create the regex pattern using % style formatting\n",
    "        # This pattern matches a string starting with 'A' followed by exactly 3 digits\n",
    "        regex_pattern = r\"%s\\d{%d}\" % (start_char, num_digits)\n",
    "\n",
    "        # Example usage with tf.strings.regex_full_match\n",
    "        input_string = tf.constant([\"A123\", \"B456\", \"A78\"])\n",
    "        matches = tf.strings.regex_full_match(input_string, regex_pattern)\n",
    "\n",
    "        print(matches)\n",
    "\n",
    "        In this example:\n",
    "\n",
    "            regex_pattern = r\"%s\\d{%d}\" % (start_char, num_digits) constructs the regex string.\n",
    "            %s is a placeholder for a string (e.g., start_char).\n",
    "            %d is a placeholder for an integer (e.g., num_digits).\n",
    "            The r prefix before the string literal ensures it's treated as a raw string, preventing backslashes from being interpreted as escape sequences by Python before the regex engine sees them. \n",
    "\n",
    "        While this works, consider using f-strings for more modern and readable code when constructing dynamic regex patterns:\n",
    "        Python\n",
    "\n",
    "        import tensorflow as tf\n",
    "\n",
    "        num_digits = 3\n",
    "        start_char = \"A\"\n",
    "\n",
    "        # Create the regex pattern using an f-string\n",
    "        regex_pattern_fstring = rf\"{start_char}\\d{{{num_digits}}}\"\n",
    "\n",
    "        input_string = tf.constant([\"A123\", \"B456\", \"A78\"])\n",
    "        matches_fstring = tf.strings.regex_full_match(input_string, regex_pattern_fstring)\n",
    "\n",
    "        print(matches_fstring)\n",
    "\n",
    "        In the f-string example, {{ and }} are used to escape the literal curly braces within the f-string, as they are part of the regex syntax for repetition.\n",
    "\n",
    "### GlobalAverage1d layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d7ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 20:40:36.768231: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-01 20:40:36.836799: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-01 20:40:38.181092: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ True False False], shape=(3,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 20:40:39.503275: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-12-01 20:40:39.503305: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:171] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-12-01 20:40:39.503323: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:176] retrieving CUDA diagnostic information for host: 2778c3d2ac10\n",
      "2025-12-01 20:40:39.503326: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] hostname: 2778c3d2ac10\n",
      "2025-12-01 20:40:39.503422: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] libcuda reported version is: 575.64.3\n",
      "2025-12-01 20:40:39.503437: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:194] kernel reported version is: 575.64.3\n",
      "2025-12-01 20:40:39.503439: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:284] kernel version seems to match DSO: 575.64.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "strings = tf.constant([\"apple\", \"banana\", \"apricot\"])\n",
    "pattern = r\"a.*e\"\n",
    "matches = tf.strings.regex_full_match(strings, pattern)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d605f98",
   "metadata": {},
   "source": [
    "## globalAveragePool1D layer.\n",
    "\n",
    " You can think of the pooling layers as a way to downsample (a way to reduce the size of) the incoming feature vectors.\n",
    "\n",
    "In the case of max pooling you take the maximum value of all features in the pool for each feature dimension. In the case of average pooling you take the average, but max pooling seems to be more commonly used as it highlights large values.\n",
    "\n",
    "Global max/average pooling takes the maximum/average of all features whereas in the other case you have to define the pool size. Keras has again its own layer that you can add in the sequential model:\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/75067335/what-does-globalaveragepooling1d-do-in-keras\n",
    "\n",
    "        After passing the input sequences through an embedding layer, we get a 3D floating-point tensor with shape (samples, sequence_length, embedding_dimensionality). Example, shape is (2, 3, 5), indicating that there are 2 samples (sequences), each with a sequence length of 3 and embedding dimensionality of 5.\n",
    "\n",
    "        However, dense layers in neural networks require fixed-length input vectors. GlobalAveragePooling1D effectively reduces the sequence dimension, creating a single vector that summarizes the information from the entire sequence.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        Embeddings:\n",
    "        word1: [0.2, 0.4, -0.1, 0.3]\n",
    "        word2: [0.1, -0.3, 0.2, 0.5]\n",
    "        word3: [-0.2, 0.1, -0.5, 0.4]\n",
    "        word4: [0.3, -0.2, 0.3, -0.1]\n",
    "        word5: [-0.4, 0.3, 0.1, -0.2]\n",
    "\n",
    "        Global Average Pooled Vector: [0, 0.06, 0, 0.09]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        The tf.keras.layers.GlobalAveragePooling1D layer's input is in the example a tensor of batch x sequence x embedding_size.\n",
    "\n",
    "        It returns a matrix of batch x embedding_size, by averaging over the sequence dimension. The average is only over one dimension therefore the 1D.\n",
    "\n",
    "        The averaging can handle handle different sequence sizes. For example sentences of any length.\n",
    "\n",
    "\n",
    "https://www.sapien.io/glossary/definition/global-pooling#:~:text=Global%20pooling%20layers%20are%20used,of%20the%20input%20image%20size.\n",
    "\n",
    "        In global average pooling, the average value of each feature map is computed, resulting in a single scalar per map. This technique helps in maintaining spatial information and is less prone to overfitting compared to fully connected layers, which have more parameters.\n",
    "\n",
    "        In global max pooling, the maximum value in each feature map is selected, capturing the most prominent feature detected by the convolutional filters. This method is useful for identifying the most significant feature in each map, which might be critical for certain classification tasks.\n",
    "\n",
    "        Global pooling is particularly beneficial in deep learning architectures where it simplifies the model, reduces the number of parameters, and enhances generalization by preventing overfitting. It also makes the model invariant to the input size, which is useful for handling images of different sizes.\n",
    "\n",
    "\n",
    "### feature concept? in the NN\n",
    "\n",
    "feature and feature map in the text embedding. \n",
    "\n",
    "it is like what we see in sequencing count matrix\n",
    "\n",
    "it is sample x gene (sample in rows and gene in columns. Here gene is the feature.)\n",
    "\n",
    "in this case, feature map is the sample x sequence x embedding. after globalAveragePool1D, we have\n",
    "sequence x embedding (averaged or maximized).\n",
    "\n",
    "the variable-length sequence dimension is reduced and have fixed length.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
