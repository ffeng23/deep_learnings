---
title: "Deep learning, part 8, transfer learning."
author: Feng, Feng
date: 12/3/2025
format: 
  html:
    embed-resources: true
    code-fold: false
    html-math-method: mathjax
jupyter: python3
---

Aims:

 + Transfer learning with examples

Ref: https://www.geeksforgeeks.org/machine-learning/ml-introduction-to-transfer-learning/


Concepts:

Transfer learning is a machine learning technique where a model trained on one task is repurposed as the foundation for a second task. This approach is beneficial when the second task is related to the first or when data for the second task is limited.

Transfer learning offers solutions to key challenges like:

Limited Data: Acquiring extensive labelled data is often challenging and costly. Transfer learning enables us to use pre-trained models, reducing the dependency on large datasets.
Enhanced Performance: Starting with a pre-trained model which has already learned from substantial data allows for faster and more accurate results on new tasks ideal for applications needing high accuracy and efficiency.
Time and Cost Efficiency: Transfer learning shortens training time and conserves resources by utilizing existing models hence eliminating the need for training from scratch.
Adaptability: Models trained on one task can be fine-tuned for related tasks making transfer learning versatile for various applications from image recognition to natural language processing.

Working of Transfer Learning
Transfer learning involves a structured process to use existing knowledge from a pre-trained model for new tasks:

Pre-trained Model: Start with a model already trained on a large dataset for a specific task. This pre-trained model has learned general features and patterns that are relevant across related tasks.
Base Model: This pre-trained model, known as the base model, includes layers that have processed data to learn hierarchical representations, capturing low-level to complex features.
Transfer Layers: Identify layers within the base model that hold generic information applicable to both the original and new tasks. These layers often near the top of the network capture broad, reusable features.
Fine-tuning: Fine-tune these selected layers with data from the new task. This process helps retain the pre-trained knowledge while adjusting parameters to meet the specific requirements of the new task, improving accuracy and adaptability.

```{mermaid}

flowchart LR

A[Task A] --> B(Pretrained Model)-->C{Knowledge}-->D(New Model)-->E[Task B]

```

Low-level features learned for task A should be beneficial for learning of model for task B.

## Frozeon vs trainable layers

Frozen Vs Trainable Layers in Transfer Learning

Aspect	 | Frozen Layers	|  Trainable Layers
---------|------------------|------------------
Definition|	Layers whose weights are kept fixed and not updated during training |	Layers whose weights are updated during training
Purpose	| Preserve general features learned from large pre-trained datasets|	Adapt to task-specific features of the new dataset
Learning Process|	No backpropagation updates; remain constant|	Updated through backpropagation based on new data
Use Case |	Used when new dataset is small or similar to the original dataset |	Used when new dataset is large or significantly different from the original task
Computation Cost |	Lower, since fewer parameters are trained	 | Higher, as more parameters need to be updated
Example in CNN |	Early convolutional layers that capture edges, textures and basic shapes |	Later fully connected layers or deeper convolutional layers for fine-tuned features


## How to Decide Which Layers to Freeze or Train?

The extent to which you freeze or fine-tune layers depends on the similarity and size of your target dataset:

 + Small, Similar Dataset: For smaller datasets that resemble the original dataset, you freeze most layers and only fine-tune the last one or two layers to prevent overfitting.

 + Large, Similar Dataset: With large, similar datasets you can unfreeze more layers allowing the model to adapt while retaining learned features from the base model.

 + Small, Different Dataset: For smaller, dissimilar datasets, fine-tuning layers closer to the input layer helps the model learn task-specific features from scratch.

 + Large, Different Dataset: In this case, fine-tuning the entire model helps the model adapt to the new task while using the broad knowledge from the pre-trained model.


# Transfer Learning with MobileNetV2 for MNIST Classification

In this section, we’ll explore transfer learning by fine-tuning a MobileNetV2 model pre-trained on ImageNet for classifying MNIST digits.

1. Preparing the Dataset
We start by loading the MNIST dataset. Since MobileNetV2 is pre-trained on three-channel RGB images of size 224x224, we make a few adjustments to match its expected input shape:

    Reshape the images from grayscale (28x28, 1 channel) to RGB (28x28, 3 channels).
    
    Resize images to 32x32 pixels, aligning with our model’s configuration.
    
    Normalize pixel values to fall between 0 and 1 by dividing by 255.

    (how???)

```{python data}

from tensorflow.keras.datasets import mnist
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import to_categorical

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = np.stack([train_images]*3, axis=-1) / 255.0 #make it from 1 channel to 3 channels.
test_images = np.stack([test_images]*3, axis=-1) / 255.0

train_images = tf.image.resize(train_images, [32, 32])
test_images = tf.image.resize(test_images, [32, 32])

train_labels = to_categorical(train_labels, 10) #turn into on hot encoding
test_labels = to_categorical(test_labels, 10)

```

We load MobileNetV2 with pre-trained weights from ImageNet excluding the fully connected top layers to customize for our 10-class classification task:

Freeze the base model to retain learned features and avoid overfitting.
Add a global average pooling layer to reduce model complexity.
Add a dense layer with softmax activation for the output classes.


```{python models_first}
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))
base_model.trainable = False  # Freeze base model

inputs = Input(shape=(32, 32, 3))

x = base_model(inputs, training=False) 
#this makes a basemodel to take in input with a shpe defined 32x32x3

#now we want to further define a new layer, globalAveragepool2d to reduce
# the dimension. and then this will take in the previous base model and 
# average globally into 1 (no matter what, see below). It keeps the channel
# this is like create the layer and process the input and generate output
#
gap=GlobalAveragePooling2D()
x = gap(x)

#similar to above, make the model pass through another layer!! meaning
#adding a layer to the model.
dl=Dense(10, activation='softmax')
outputs = dl(x)
model = Model(inputs, outputs)

model.summary()
```

**The GlobalAveragePooling2D layer** computes the average of all values in each feature map. This reduces the spatial dimensions (height and width) to 1 while retaining the depth (number of channels). The resulting output is a 2D tensor with shape (batch_size, channels).

train the model

```{python model_2nd}

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=10, validation_split=0.2)

```

Fine-Tuning the Model
After initial training we unfreeze the last few layers of the base model to perform fine-tuning. This allows the model to adjust high-level features for the MNIST data while retaining its foundational knowledge.

```{python model_finetune}
base_model.trainable = True

#there are about 153 layers in total. top 50s are trainable!!!
for layer in base_model.layers[:100]:
    layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5, validation_split=0.2)

```

Evaluate the model


```{python evaluate}

loss, accuracy = model.evaluate(test_images, test_labels)
print(f"Test loss: {loss}")
print(f"Test accuracy: {accuracy}")


from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

test_predictions = model.predict(test_images)
test_predictions_classes = np.argmax(test_predictions, axis=1)
test_true_classes = np.argmax(test_labels, axis=1)

cm = confusion_matrix(test_true_classes, test_predictions_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

```

visualize samples

```{python sample_vis}

def display_sample(sample_images, sample_labels, sample_predictions):
    fig, axes = plt.subplots(3, 3, figsize=(12, 12))
    fig.subplots_adjust(hspace=0.5, wspace=0.5)

    for i, ax in enumerate(axes.flat):
        ax.imshow(sample_images[i].reshape(32, 32), cmap='gray')
        ax.set_xlabel(f"True: {sample_labels[i]}\nPredicted: {sample_predictions[i]}")
        ax.set_xticks([])
        ax.set_yticks([])

    plt.show()

test_images_gray = np.dot(test_images[...,:3], [0.2989, 0.5870, 0.1140])

random_indices = np.random.choice(len(test_images_gray), 9, replace=False)
sample_images = test_images_gray[random_indices]
sample_labels = test_true_classes[random_indices]
sample_predictions = test_predictions_classes[random_indices]
display_sample(sample_images, sample_labels, sample_predictions)

```

# Thoughts

def display_sample(sample_images, sample_labels, sample_predictions):
    fig, axes = plt.subplots(3, 3, figsize=(12, 12))
    fig.subplots_adjust(hspace=0.5, wspace=0.5)

    for i, ax in enumerate(axes.flat):
        ax.imshow(sample_images[i].reshape(32, 32), cmap='gray')
        ax.set_xlabel(f"True: {sample_labels[i]}\nPredicted: {sample_predictions[i]}")
        ax.set_xticks([])
        ax.set_yticks([])

    plt.show()

test_images_gray = np.dot(test_images[...,:3], [0.2989, 0.5870, 0.1140])

random_indices = np.random.choice(len(test_images_gray), 9, replace=False)
sample_images = test_images_gray[random_indices]
sample_labels = test_true_classes[random_indices]
sample_predictions = test_predictions_classes[random_indices]
display_sample(sample_images, sample_labels, sample_predictions)
Output:

Labeled-Images
Labelled Images Output
You can get the complete source code from here.

Applications
Transfer learning is widely used across multiple domains including:

Computer Vision: Transfer learning is prevalent in image recognition tasks where models pre-trained on large image datasets are adapted to specific tasks such as medical imaging, facial recognition and object detection.
Natural Language Processing (NLP): In NLP models like BERT, GPT or ELMo are pre-trained on vast text corpora and later fine-tuned for specific tasks such as sentiment analysis, machine translation and question-answering.
Healthcare: Transfer learning helps develop medical diagnostic tools using knowledge from general image recognition models to analyze medical images like X-rays or MRIs.
Finance: Transfer learning in finance assists in fraud detection, risk assessment and credit scoring by transferring patterns learned from related financial datasets.
Advantages
Speed up the training process: By using a pre-trained model the model can learn more quickly and effectively on the second task, as it already has a good understanding of the features and patterns in the data.
Better performance: Transfer learning can lead to better performance on the second task, as the model can use the knowledge it has gained from the first task.
Handling small datasets: When there is limited data available for the second task, transfer learning can help to prevent overfitting as the model will have already learned general features that are likely to be useful in the second task.
Disadvantages
Domain mismatch: The pre-trained model may not be well-suited to the second task if the two tasks are vastly different or the data distribution between the two tasks is very different.
Overfitting: Transfer learning can lead to overfitting if the model is fine-tuned too much on the second task, as it may learn task-specific features that do not generalize well to new data.
Complexity: The pre-trained model and the fine-tuning process can be computationally expensive and may require specialized hardware.


# Extra exercise.

I want to see whether I can rewrite the nn for building image analysis

using the mnist data.

```{python sample_extra}

(train_samples,train_cats),(test_samples,test_cats)=mnist.load_data()

#normalize the data
train_samples=train_samples/255.0
test_samples=test_samples/255.0

#categorical
train_cats=to_categorical(train_cats)
test_cats=to_categorical(test_cats)
```

Build the model with one hidden layer!!!

+ flatten layer first 

+ hidden layer with 128 nodes

+ output layer with 10 nodes

```{python model_extra2}

input=Input(shape=(28,28))
model_extra=tf.keras.Sequential()



model_extra.add(tf.keras.layers.Flatten(input_shape=(28,28)))

model_extra.add(tf.keras.layers.Dense(32, activation='relu'))

model_extra.add(tf.keras.layers.Dense(10, activation='relu'))
model_extra.add(tf.keras.layers.Dense(10, activation='softmax'))


model_extra.summary()

```

compile model

```{python compile_extra}

model_extra.compile(loss="categorical_crossentropy", optimizer="adam",
  metrics=["accuracy"]
)

model_extra.fit(test_samples, test_cats, epochs=40, validation_split=0.2)

```

