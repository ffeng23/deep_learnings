---
title: Deep learning part 4, ML beginner - Image classification.
author: Feng, Feng
date: 11/29/2025
format:
  html:
    embed-resources: True
    html-math-method: mathjax
    code-fold: False
jupyter: python3
---

Aim:
  + Continue to learn keras, machine learning and python

part 4 from https://www.tensorflow.org/tutorials/keras/classification

Image classification.

## Basic classification

prepare

```{python prepare}

# TensorFlow and tf.keras
import tensorflow as tf

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print("TensorFlow version:", tf.__version__)
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

print("Keras version:", tf.keras.__version__)
```

Data, Fashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the "Hello, World" of machine learning programs for computer vision.

```{python data}

fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

#show data type and shape
print(f"train_images type: {type(train_images)} and {train_images.shape}")
print(f"train_label type and shape: {type(train_labels)} and {train_labels.shape}")

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

```

precess data

```{python precess}

plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()

```

```{python process2}

train_images = train_images / 255.0

test_images = test_images / 255.0

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()


```

build the model

```{python model1}
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

```

The first layer in this network, **tf.keras.layers.Flatten**, transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has **no parameters to learn**; it only reformats the data.

```{python model2}

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=10)

```

evaluate

```{python evaluate1}

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print('\nTest accuracy:', test_acc)

```

Make predication. Add the a layer to the model to turn logits into probs.

```{python predict}

predict_model=tf.keras.Sequential([model, tf.keras.layers.Softmax()])


pre=predict_model(test_images[1:5])

index=np.argmax(pre, axis=1).tolist()

pre_name=[class_names[i] for i in index]
```

Now do some visualization

```{python vis1}
def plot_image(i, predictions_array, true_label, img):
  true_label, img = true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  """
  i, interge the ith element in the sample to visualize
  predictions_array, 1-d tensor array, the ith prediction output, shape of [10]

  true_label, tuple, holding all true labels for test data, 10000x1
  """
  true_label = true_label[i]
  plt.grid(False)
  plt.xticks(range(10))
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')


i = 0
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)

pre_i=predict_model(test_images)
plot_value_array(i, pre_i[i], test_labels)

plot_image(i, pre_i[i], test_labels, test_images)

#doing another one

i = 12
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, pre_i[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, pre_i[i],  test_labels)
plt.show()


```

show the fist a few.

```{python vis3}

# Plot the first X test images, their predicted labels, and the true labels.
# Color correct predictions in blue and incorrect predictions in red.
num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, pre_i[i], test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, pre_i[i], test_labels)
plt.tight_layout()
plt.show()

```

Use the trained model
```{python use1}
# Grab an image from the test dataset.
img = test_images[1]

print(img.shape)

```

tf.keras models are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:

```{python use2}
# Add the image to a batch where it's the only member.
img = (np.expand_dims(img,0))

print(img.shape)

predictions_single = predict_model.predict(img)

print(predictions_single)

plot_value_array(1, predictions_single[0], test_labels)
_ = plt.xticks(range(10), class_names, rotation=45)
plt.show()

```

## layers drop out

### co-adaptation

When a fully-connected layer has a large number of neurons, **co-adaptation** is more likely to happen. Co-adaptation refers to when multiple neurons in a layer extract the same, or very similar, hidden features from the input data. This can happen when the connection weights for two different neurons are nearly identical.

An example of co-adaptation between neurons A and B. Due to identical weights, A and B will pass the same value into C.

This poses two different problems to our model:

Wastage of machine's resources when computing the same output.
If many neurons are extracting the same features, it adds more significance to those features for our model. This leads to overfitting if the duplicate extracted features are specific to only the training set.

Solution to the problem: As the title suggests, we use dropout while training the NN to minimize co-adaptation. In dropout, we randomly shut down some fraction of a layer’s neurons at each training step by zeroing out the neuron values. The fraction of neurons to be zeroed out is known as the dropout rate, $r_d$. 


The remaining neurons have their values multiplied by 

$$
  \frac{1}{1-r_d}
$$​
      
so that the overall sum of the neuron values remains the same. 

 



The two images represent dropout applied to a layer of 6 units, shown at multiple training steps. The dropout rate is 1/3, and the remaining 4 neurons at each training step have their value scaled by x1.5. Thereby, we are choosing a random sample of neurons rather than training the whole network at once. This ensures that the co-adaptation is solved and they learn the hidden features better. 

Why dropout works?

By using dropout, in every iteration, you will work on a smaller neural network than the previous one and therefore, it approaches regularization.
Dropout helps in shrinking the squared norm of the weights and this tends to a reduction in overfitting.
Dropout can be applied to a network using TensorFlow APIs as follows: 




    tf.keras.layers.Dropout(
        rate
    )
    ​
    # rate: Float between 0 and 1.
    # The fraction of the input units to drop.





