---
title: Implementation of NN
author: Feng, Feng
date: 11/6/2025
format:
  html:
    html-math-method: mathjax
    code-fold: false
    embed-resources: true
jupyter: python3
---

# Aims

+ practising implementation of NN
+ deepening understanding of NN and machine learning.

reference: https://dennybritz.com/posts/wildml/implementing-a-neural-network-from-scratch/

# Start Implementing

## Data generation.

Make use of a scikit-learn function, make_moons

```{python data_1}
from sklearn import datasets 
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

np.random.seed(2025)

X, Y= datasets.make_moons(200, noise=0.2)
y=Y #feng, added because I modified the original code to have Y instead of Y
print(f"X.shape : {X.shape} and Y.shape : {Y.shape}\n")

print(f"Show the data X, \n {X[1:10,:]}\n")

print(f"Show the data Y: \n{Y[1:10,]}")

plt.figure(figsize=(6,4))

sns.scatterplot(x=X[:,0], y=X[:,1], c=Y)
plt.show()

```


## Logistic regression as the classifier.   

Try the logistic regression to classify the data for the demonstration purpose.

```{python logistic}

import sklearn.linear_model 

clf=sklearn.linear_model.LogisticRegressionCV(random_state=20)

clf.fit(X,Y)

print(f"after fitting: we have {clf}")

def plot_decision_boundary(pred_func, X, y):
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)
    #plt.show()

# Plot the decision boundary
plot_decision_boundary(lambda x: clf.predict(x), X, Y)


```

## Implementation of a light-weight NN

with 2 layers, one hidden layer and one output layer (2 nodes). The input layer is of two nodes (x) too.

Defining the variables first,

```{python variables}

num_examples = len(X) # training set size
nn_input_dim = 2 # input layer dimensionality
nn_output_dim = 2 # output layer dimensionality

# Gradient descent parameters (I picked these by hand)
epsilon = 0.01 # learning rate for gradient descent
reg_lambda = 0.01 # regularization strength

print(f"Sample size: {num_examples}")


```

loss function

```{python loss}

#copy from this post
#https://insidelearningmachines.com/cross_entropy_loss/
# modified to fit this module
import numpy as np


#this one is not going to work!!!!!!, I added based on the one below it. The one
# below this one is from the project and it assumes the one-hot coding, while the
# data is not, with data y is showing the labeling/classification only.
#DON"T RUN THIS!!! It is not going to work.!!!!
def cross_entropy(model, X, y) -> float:
    """
    Function to compute cross entropy for distributions y_true & y_pred
    
    Input:
        y_true : numpy array of true labels
        y_pred : numpy array of model predictions
        
    Output:
        scalar cross entropy value between y_true & y_pred
    """
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    # Forward propagation to calculate our predictions
    z1 = X.dot(W1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(W2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

    offset = 1e-16

    # Calculating the loss, No!! y is 200x1, but probs is 200x2
    corect_logprobs = [-1*y_t*np.log(y_p+offset) for y_t, y_p in zip(y,probs)]
    
    # if we want to make it work, in this specific case, we simple add
    # all log(y_p). that will do the job. since this case, if we 
    # change to one-hot code, y_p will be the one with y_t ==1.
    #corect_logprobs = -1*np.log(y_p+offset)
    data_loss = np.sum(corect_logprobs)
    # Add regulatization term to loss (optional)
    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))
    return 1./num_examples * data_loss


#in the impolementing post
def calculate_loss(model):
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    # Forward propagation to calculate our predictions
    z1 = X.dot(W1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(W2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    # Calculating the loss
    corect_logprobs = -np.log(probs[range(num_examples), y])
    #this  is a good coding line. it is a little confusing, because
    # probs are using one-hot coding, but y is not. so here, y is
    # the true label, therefore, when we try calulate, we only consider
    # true label probs, but not the wrong probs, since wrong probs will n
    # need to multiply with 0 (wrong lab) and end up with no contribution!!!

    data_loss = np.sum(corect_logprobs)
    # Add regulatization term to loss (optional)
    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))
    return 1./num_examples * data_loss

```

Try to figure out the dimension in this calculation!!

Here, we have 2 nodes for input x, 2 nodes of the only hidden layer, 2 nodes for output (for one-hot coding):

    X is (200,2)
    W1 is (2,2)  # two nodes by 2 parameters,
    b1 is (1,2)
    z1 is (200,2)
    a1 is (200,2)

    W2 is (2,2)
    b2 is (1,2)
    z2 is (200,2)
    a2 or output prob is (200,2) 

Note: 200 is number of samples, 2 is the number of nodes in each layer

 In the a2 or output, there is one-hot coding. we have two possible labels for each sample output, p0 and p1.
 Note again, when calculate loss, only the estimated probability for the correct label (y=1) contribute, but the wrong label probability doesn't help (y=0, y*log(p0)=0).


Now we want to deviate a little to test and get familiar with cross-entropy loss.

```{python loss_test}


def cross_entropy_test(y_true: np.array, y_pred: np.array) -> float:
    """
    Function to compute cross entropy for distributions y_true & y_pred
    
    Input:
        y_true : numpy array of true labels
        y_pred : numpy array of model predictions
        
    Output:
        scalar cross entropy value between y_true & y_pred
    """
    offset = 1e-16

    #note!!!: this below is the typical formula for 
    # two-class cross entropy loss for non-one-hot coding.
    #  the y coding is one column showing what that class is,
    # either 0 or 1.
    return -np.sum([y_t*np.log2(y_p + offset)+(1-y_t)*np.log2(1-y_p+offset) for y_t,y_p in zip(y_true,y_pred)])

# two class example no. 1
y_true = np.array([1.0, 0.0, 0.0, 1.0])
y_pred = np.array([0.8, 0.5, 0.6, 0.4])

print(f'Cross Entropy for example 1 is: {cross_entropy_test(y_true,y_pred):.2f}')

# sample no. 2
y_true = np.array([1.0, 0.0, 0.0, 0.0])
y_pred = np.array([1, 1, 1, 1])

print(f'Cross Entropy for example 2 is: {cross_entropy_test(y_true,y_pred):.2f}')

y_true = np.array([0.0, 0.0, 0.0, 0.0])
y_pred = np.array([0.8, 0.5, 0.6, 0.4])

print(f'Cross Entropy for example 1 is: {cross_entropy_test(y_true,y_pred):.2f}')

y_true = np.array([1.0, 0.0, 0.0, 0.0])
y_pred = np.array([0.0, 0.0, 0.0, 0.0])

print(f'Cross Entropy for example 1 is: {cross_entropy_test(y_true,y_pred):.2f}')

y_true = np.array([0.0, 0.0, 0.0, 0.0])
y_pred = np.array([0.0, 0.0, 0.0, 0.0])

print(f'Cross Entropy for example 1 is: {cross_entropy_test(y_true,y_pred):.2f}')


```

tensor flow testing for ONE-HOT-ENCODING output.

```{python tensor_test}

import tensorflow as tf

y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]

cce = tf.keras.losses.CategoricalCrossentropy()
loss = cce(y_true, y_pred)
print(loss.numpy()) # Output: 1.177

print(f"doing binary response0------------")
y_true = [[0, 1], [ 0, 1]]
y_pred = [[ 0.1, 0.9], [ 0.2, 0.8]]
loss = cce(y_true, y_pred)
print(f"all label 1 {loss.numpy()}") # Output: 0.164

print("all wrong label:")
y_true = [[0, 1], [ 0, 1]]
y_pred = [[ 0.95, 0.05], [ 0.8, 0.2]]
loss = cce(y_true, y_pred)
print(loss.numpy()) #2.3025851

type(y_true)

# now try manually calculate the above loss
y_true_np=np.array(y_true)
y_pred_np=np.array(y_pred)

offset=1e-16
loss_manual=np.matmul(y_true_np,np.transpose(np.log(y_pred)))
loss_manual2=np.dot(y_true_np,np.transpose(np.log(y_pred)))
loss_manual3=y_true_np*np.log(y_pred)
loss_manual5=np.multiply(y_true_np, np.log(y_pred))

loss_manual4=np.sum(loss_manual3)/-2
print(loss_manual4)

print(np.sum(loss_manual5)/2)
```

Now that I understand that why in the above defined function, calculate_loss, it only calculate cross-entroyp of the y=1 cases. Because it use 1) use one-hot coding and 2) multiple class labelling. In this case, each sample has a vector of output/y, but when we do calculating, it counts everything, but only the classification of 1 matters with classification of 0 ending up to be 0= 0*log(p). 
Remember, e.g., y=[1,0,0,0] means for this sample its label/classification is the first one, and $\hat{y}=[0.9, 0,0.1,0]$ means it predicts to be first one. In this case, cross-entropy is small. Otherwise, $\hat{y}=[0.09, 0,0.91,0]$ will have $-\sum_{i=1}^{4}y^log(\hat{y})$ to be big and equals to $-1*log(0.09)$ only.

This is what tensorflow is using.!! one-hot and cross-entropy with multi-class labeling.


**dot product and element-wise multiplication**

```{python element_wise}

#element-wise and broadcasting
array1 = np.array([1, 2, 3])
array2 = np.array([[1], [2], [3]])
result = array1 * array2
print(result)

```

Now continue with the NN implementation. The next function is "predict"


```{python predict}

# Helper function to predict an output (0 or 1)
def predict(model, x):
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    # Forward propagation
    z1 = x.dot(W1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(W2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    return np.argmax(probs, axis=1) #<-softmax function in this case, 2 classese

```

Build the model for NN,

```{python model}
# Gradient descent parameters (I picked these by hand) 
# here use global variable, might not be a good idea!!
epsilon = 0.01 # learning rate for gradient descent 
reg_lambda = 0.01 # regularization strength 

# This function learns parameters for the neural network and returns the model.
# - nn_hdim: Number of nodes in the hidden layer
# - num_passes: Number of passes through the training data for gradient descent
# - print_loss: If True, print the loss every 1000 iterations
def build_model(nn_hdim, num_passes=20000, print_loss=False):
    
    # Initialize the parameters to random values. We need to learn these.
    np.random.seed(0)
    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)
    b1 = np.zeros((1, nn_hdim))
    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)
    b2 = np.zeros((1, nn_output_dim))

    # This is what we return at the end
    model = {}
    
    # Gradient descent. For each batch...
    for i in range(0, num_passes):

        # Forward propagation
        z1 = X.dot(W1) + b1
        a1 = np.tanh(z1)
        z2 = a1.dot(W2) + b2
        exp_scores = np.exp(z2)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

        # Backpropagation
        delta3 = probs
        delta3[range(num_examples), y] -= 1
        # Feng's note: here we need to be careful, this is correct
        # delta3 is of shape (200, 2)
        # it basically calculate element-wise subtraction
        #    A-Y, 
        # for one-hot coding, y=1, where the correct label, \hat(p) =1
        #  cases we need to do \hat(p)-1. where the wronge label,
        # \hat(p)=0, we don't do calculation, since p-0=p.
        # so the first one take cares of the wrong labels outputs,
        # and second line we have the correct label cases.
        # overall the vectorized formulus is A-Y, for softmax function 
        #
        dW2 = (a1.T).dot(delta3)
        db2 = np.sum(delta3, axis=0, keepdims=True)
        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))
        dW1 = np.dot(X.T, delta2)
        db1 = np.sum(delta2, axis=0)

        # Add regularization terms (b1 and b2 don't have regularization terms)
        dW2 += reg_lambda * W2
        dW1 += reg_lambda * W1

        # Gradient descent parameter update
        W1 += -epsilon * dW1
        b1 += -epsilon * db1
        W2 += -epsilon * dW2
        b2 += -epsilon * db2
        
        # Assign new parameters to the model
        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}
        
        # Optionally print the loss.
        # This is expensive because it uses the whole dataset, so we don't want to do it too often.
        if print_loss and i % 1000 == 0:
          print ("Loss after iteration %i: %f" %(i, calculate_loss(model)))
    
    return model

```


Now lets try to run and see how it works!!!

```{python run_model}

# Build a model with a 3-dimensional hidden layer
model = build_model(3, print_loss=True)


# Plot the decision boundary
plot_decision_boundary(lambda x: predict(model, x), X, y)
plt.title("Decision Boundary for hidden layer size 3")


# %% 14 
plt.figure(figsize=(16, 32)) 
hidden_layer_dimensions = [1, 2]#, 3, 4, 5, 20, 50] 
for i, nn_hdim in enumerate(hidden_layer_dimensions): 
    plt.subplot(5, 2, i+1) 
    plt.title('Hidden Layer size %d' % nn_hdim) 
    model = build_model(nn_hdim) 
    plot_decision_boundary(lambda x: predict(model, x),X, y) 
plt.show()

```

# building nn using tensorflow

https://pythonguides.com/training-neural-network-in-tensorflow/

build a neural network with tensorflow to classify data.

check version first,
```{python tensor_1}
import tensorflow as tf
# Check TensorFlow version
print(f"TensorFlow version: {tf.__version__}")
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

```

Load data

```{python tenso_adat}
import keras
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Normalize pixel values to be between 0 and 1
x_train, x_test = x_train / 255.0, x_test / 255.0

# Build the model
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

```

train the model

```{python train}

# Train the model
history = model.fit(
    x_train, y_train,
    epochs=10,
    validation_data=(x_test, y_test)
)

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f'\nTest accuracy: {test_acc}')

```

visualize the fitting

```{python vis}

# Plot training & validation accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='lower right')

# Plot training & validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.tight_layout()
plt.show()

```

# now try keras on moon data

Prepare the data

First we want to try without split and without normalizing/scaling them
```{python moon_data}

moon_train_x=X
moon_train_y=y

```

Build the model

```{python moon_model}

moon_model= keras.Sequential([
    #now need to flatten the data (why need that?), since the data is 1D, vector of 2 for each sample

    keras.layers.Dense(2, activation='relu',input_shape=(2,)),#2 nodes for this layer, input shape is 2
    keras.layers.Dense(2,activation='softmax')

]
)

moon_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)


```

train the model

```{python moon_training}


# Train the model
history_moon = moon_model.fit(
    moon_train_x, moon_train_y,
    epochs=2000,
    validation_data=(moon_train_x, moon_train_y)
)

# Evaluate the model
test_loss, test_acc = moon_model.evaluate(moon_train_x, moon_train_y, verbose=2)
print(f'\nTest accuracy: {test_acc}')


```

visualize the classification

```{python vis_moon_keras}

# Helper function to predict an output (0 or 1)
def moon_predict(model, x):
    
    return np.argmax(model.predict(x), axis=1) #<-softmax function in this case, 2 classese

plot_decision_boundary(lambda x: moon_predict(moon_model,x), X, y)



```

now try nodes of 3

```{python model_moon3}


moon_model= keras.Sequential([
    #now need to flatten the data (why need that?), since the data is 1D, vector of 2 for each sample

    keras.layers.Dense(1400, activation='tanh',input_shape=(2,)),#2 nodes for this layer, input shape is 2
    keras.layers.Dense(2,activation='softmax')

]
)

moon_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)



# Train the model
history_moon = moon_model.fit(
    moon_train_x, moon_train_y,
    epochs=2000,
    validation_data=(moon_train_x, moon_train_y)
)

# Evaluate the model
test_loss, test_acc = moon_model.evaluate(moon_train_x, moon_train_y, verbose=2)
print(f'\nTest accuracy: {test_acc}')



plot_decision_boundary(lambda x: moon_predict(moon_model,x), X, y)

```

Conclusion about complexity (# of nodes in the layer): it seems that keras has some way of controlling over-fitting. It won't be that crazy like how the #nodes affect the fitting in the simple NN manually implemented above. Even when the node # goes up to 1400 it won't be too curvy.

For this simple question, it won't matter too much to select between tanh or relu.

Running more epoches might squeeze the performance better tahn other parameters.


## try one other example

linear regression???

```{python house_data}

# Load California housing dataset
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler #standardize data, mean std

# Get data
housing = fetch_california_housing()
X, y = housing.data, housing.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

```

now build model and train

```{python house_model}

# Build model for regression
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)
])

# Compile
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train
history = model.fit(
    X_train_scaled, y_train,
    epochs=100,
    validation_split=0.2,
    verbose=2,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
    ]
)

test_loss, test_mae=model.evaluate(X_test, y_test)

print(test_loss, test_mae)


```

save the model

```{python save_model}

# Save the model
model.save('housing_model.keras')

# Later, to load the model:
loaded_model = keras.models.load_model('housing_model.keras')

```

## tips for training

            # Setup optimizer and loss function
            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

            # Training loop
            @tf.function
            def train_step(x, y):
                with tf.GradientTape() as tape:
                    logits = model(x, training=True)
                    loss_value = loss_fn(y, logits)
                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))
                return loss_value

            # Example training epoch
            def train_epoch(dataset):
                total_loss = 0
                num_batches = 0
                for x_batch, y_batch in dataset:
                    loss = train_step(x_batch, y_batch)
                    total_loss += loss
                    num_batches += 1
                return total_loss / num_batches

## Other important considerations

+ Batch Normalization: Add this after dense layers to stabilize and accelerate training

        keras.layers.Dense(64, activation='relu'),
        keras.layers.BatchNormalization(),

+ Learning Rate Scheduling: Reduce the learning rate as training progresses
   
    lr_scheduler = keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001
    )
    model.fit(..., callbacks=[lr_scheduler])


+ Regularization: Combat overfitting with L1/L2 regularization
    
    keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))


Iâ€™ve found these techniques make a huge difference in real-world projects, especially when working with limited data.