---
title: "Deep learning part 7, regression."
author: "Feng, Feng"
format: 
  html:
    code-fold: false
    html-math-method: mathjax
    embed-resources: true
Jupyter: python3
---

Aims:

  + basic regression by TF keras

Ref:

https://www.tensorflow.org/tutorials/keras/regression


load libraries

```{python library}

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# Make NumPy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

print("tf version:",tf.__version__)
print("tf.keras version:", tf.keras.__version__)

print("GPU:", tf.config.list_physical_devices("GPU"))


```

read data

```{python data}

url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',
                'Acceleration', 'Model Year', 'Origin']

raw_dataset = pd.read_csv(url, names=column_names,
                          na_values='?', comment='\t',
                          sep=' ', skipinitialspace=True)

raw_dataset.head()
raw_dataset.columns

dataset = raw_dataset.copy()
dataset.tail()

dataset.isna().sum()

dataset = dataset.dropna()

print(f"after dropping na:{dataset.shape}, and before {raw_dataset.shape}")
```

The "Origin" column is categorical, not numeric. So the next step is to **one-hot encode** the values in the column with pd.get_dummies.

Note: You can set up the tf.keras.Model to do this kind of transformation for you but that's beyond the scope of this tutorial. Check out the Classify structured data using Keras preprocessing layers or Load CSV data tutorials for examples.

```{python dummy}
dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})

dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')
dataset.tail()


train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index) # get rid of trained dataset and left with test ones.

```

explore the data
```{python explore}

sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight','Japan']], diag_kind='kde')

train_dataset.describe().transpose()

```

Split features from labels
Separate the target value—the "label"—from the features. This label is the value that you will train the model to predict.

```{python features}
train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('MPG')
test_labels = test_features.pop('MPG')
```

normalization

first see how differen they are for features

  + **It is good practice to normalize features that use different scales and ranges.**

  + Although a model might converge without feature normalization, normalization makes training much more stable.

  + There is no advantage to normalizing the one-hot features—it is done here for simplicity. 

```{python normalization}
train_dataset.describe().transpose()[['mean', 'std']]


```

The tf.keras.layers.Normalization is a clean and simple way to add feature normalization into your model.

The first step is to create the layer:

```{python first4}
normalizer = tf.keras.layers.Normalization(axis=-1) #this is saying to normalize along the last index, usually feature dimension
```
Then, fit the state of the preprocessing layer to the data by calling Normalization.adapt:

```{python first3}
normalizer.adapt(np.array(train_features))
```
Calculate the mean and variance, and store them in the layer by adapt:

```{python first2}
print(normalizer.mean.numpy())
```
When the layer is called, it returns the input data, with each feature independently normalized:

```{python first}
first = np.array(train_features[:1], dtype='float32')

first_int = np.array(train_features[:1])

with np.printoptions(precision=2, suppress=True):
  print('First example:', first)
  print()
  print('Normalized:', normalizer(tf.convert_to_tensor(first_int,dtype=tf.float32, name='tensor1')))

  print('Normalized:', normalizer(first).numpy())
```


Linear regression with one variable
Begin with a single-variable linear regression to predict 'MPG' from 'Horsepower'.

  + Normalize the 'Horsepower' input features using the tf.keras.layers.Normalization preprocessing layer.

  + Apply a linear transformation (
  ) to produce 1 output using a linear layer (tf.keras.layers.Dense).

```{python model}
horsepower = np.array(train_features['Horsepower'])

horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)
horsepower_normalizer.adapt(horsepower)

horsepower_model = tf.keras.Sequential([
    horsepower_normalizer,
    layers.Dense(units=1)
])

horsepower_model.summary()

```

run untrained model

```{python run1}

#Run the untrained model on the first 10 'Horsepower' values. The output won't be good, but notice that it has the expected shape of (10, 1):


horsepower_model.predict(horsepower[:10])

```

train

```{python train1}
horsepower_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

##%%time
history = horsepower_model.fit(
    train_features['Horsepower'],
    train_labels,
    epochs=100,
    # Suppress logging.
    verbose=2,
    # Calculate validation results on 20% of the training data.
    validation_split = 0.2)
```

visualize training steps

```{python train_vis}

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
print(hist.tail())

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)

plot_loss(history)

test_results = {}

test_results['horsepower_model'] = horsepower_model.evaluate(
    test_features['Horsepower'],
    test_labels, verbose=0)
```

```{python single_plot_line}

def plot_horsepower(x, y):
  plt.scatter(train_features['Horsepower'], train_labels, label='Data')
  plt.plot(x, y, color='k', label='Predictions')
  plt.xlabel('Horsepower')
  plt.ylabel('MPG')
  plt.legend()

x = tf.linspace(0.0, 250, 251)
y = horsepower_model.predict(x)

plot_horsepower(x,y)
```

# regression with multiple variables

multiple features

```{python model_multi}

linear_model = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1) #here note: a dense layer without activation func, meaning linear relationship.!!!!!This is how regression works.
])

print(train_features[:10])
linear_model.predict(train_features[:10])
```

check the weights

```{python kernel}

linear_model.layers[1].kernel


linear_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

history = linear_model.fit(
    train_features,
    train_labels,
    epochs=100,
    # Suppress logging.
    verbose=2,
    # Calculate validation results on 20% of the training data.
    validation_split = 0.2)

```

visualize

```{python vis}

plot_loss(history)

test_results['linear_model'] = linear_model.evaluate(
    test_features, test_labels, verbose=0)

```

# Regression with a deep neural network (DNN)

DEEP NN

hidden layers with relu activation enables possibly a non-linear relations.
The code is basically the same except the model is expanded to include some "hidden" non-linear layers. The name "hidden" here just means not directly connected to the inputs or outputs.

These models will contain a few more layers than the linear model:

 + The normalization layer, as before (with horsepower_normalizer for a single-input model and normalizer for a multiple-input model).
 + Two hidden, non-linear, Dense layers with the ReLU (relu) activation function nonlinearity.
 + A linear Dense single-output layer.

Both models will use the same training procedure, so the compile method is included in the build_and_compile_model function below.

```{python function_nn}

def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model

```

## Regression using a DNN and a single input

```{python dnn_single}

dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)

dnn_horsepower_model.summary()

history = dnn_horsepower_model.fit(
    train_features['Horsepower'],
    train_labels,
    validation_split=0.2,
    verbose=2, epochs=100)

```

vis

```{python vis_dnn_single}

plot_loss(history)

```
This model does slightly better than the linear single-input horsepower_model:

```{python vis_single_2}

x = tf.linspace(0.0, 250, 251)
y = dnn_horsepower_model.predict(x)

plot_horsepower(x, y)

test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(
    test_features['Horsepower'], test_labels,
    verbose=0)
```

## Regression with multiple features using dnn

```{python dnn_multi_model}
dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()

#%%time
history = dnn_model.fit(
    train_features,
    train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)

plot_loss(history)

test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)
```

# performance
```{python perform}
pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T


```

# predict

```{python pre}

test_predictions = dnn_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
lims = [0, 50]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

```

variance

```{python var}

error = test_predictions - test_labels
plt.hist(error, bins=25)
plt.xlabel('Prediction Error [MPG]')
_ = plt.ylabel('Count')

```

# Conclusion and notes

 + Mean squared error (MSE) **(tf.keras.losses.MeanSquaredError) and mean absolute error (MAE) (tf.keras.losses.MeanAbsoluteError)** are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.

 + Similarly, evaluation metrics used for regression differ from classification.

 + When numeric input data features have values with different ranges, each feature should be scaled independently to the same range. **Normalization**

 + Overfitting is a common problem for DNN models, though it wasn't a problem for this tutorial. Visit the Overfit and underfit tutorial for more help with this.